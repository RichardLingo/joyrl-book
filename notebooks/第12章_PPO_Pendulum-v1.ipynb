{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 定义算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=256, std = 0):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.mu_layer = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigma =  nn.Parameter(torch.ones(1, output_dim) * std)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "    def forward(self,x):\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        mu = torch.tanh(self.mu_layer(x))\n",
    "        log_std = torch.exp(self.sigma).expand_as(mu)\n",
    "        return mu,log_std\n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self,input_dim,output_dim,hidden_dim=256):\n",
    "        super(Critic,self).__init__()\n",
    "        assert output_dim == 1 # critic must output a single value\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "    def forward(self,x):\n",
    "        x = F.tanh(self.fc1(x))\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        value = self.fc3(x)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 定义经验回放"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "class ReplayBufferQue:\n",
    "    '''经验回放池，使用deque实现'''\n",
    "    def __init__(self, capacity: int) -> None:\n",
    "        self.capacity = capacity\n",
    "        self.buffer = deque(maxlen=self.capacity)\n",
    "    def push(self,transitions):\n",
    "        '''\n",
    "        '''\n",
    "        self.buffer.append(transitions)\n",
    "    def sample(self, batch_size: int):\n",
    "        if batch_size > len(self.buffer):\n",
    "            batch_size = len(self.buffer)\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        return zip(*batch)\n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class PGReplay(ReplayBufferQue):\n",
    "    '''PG的经验回放池，每次采样所有样本，因此只需要继承ReplayBufferQue，重写sample方法即可\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.buffer = deque()\n",
    "    def sample(self, batch_size: int):\n",
    "        ''' sample all the transitions\n",
    "        '''\n",
    "        if len(self.buffer) < batch_size:\n",
    "            return None\n",
    "        batch = list(self.buffer)\n",
    "        return zip(*batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 定义智能体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions import Normal\n",
    "import numpy as np\n",
    "class Agent:\n",
    "    def __init__(self,cfg) -> None:\n",
    "        self.gamma = cfg.gamma\n",
    "        self.device = torch.device(cfg.device) \n",
    "        self.actor = Actor(cfg.n_states,cfg.n_actions, hidden_dim = cfg.actor_hidden_dim).to(self.device)\n",
    "        self.critic = Critic(cfg.n_states,1,hidden_dim=cfg.critic_hidden_dim).to(self.device)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=cfg.actor_lr)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=cfg.critic_lr)\n",
    "        self.action_space = cfg.action_space\n",
    "        self.action_scale = (self.action_space.high[0] - self.action_space.low[0])/2\n",
    "        self.action_bias = (self.action_space.high[0] + self.action_space.low[0])/2\n",
    "        self.memory = PGReplay()\n",
    "        self.k_epochs = cfg.k_epochs # update policy for K epochs\n",
    "        self.eps_clip = cfg.eps_clip # clip parameter for PPO\n",
    "        self.entropy_coef = cfg.entropy_coef # entropy coefficient\n",
    "        self.sample_count = 0\n",
    "        self.batch_size = cfg.batch_size\n",
    "\n",
    "    def sample_action(self,state):\n",
    "        self.sample_count += 1\n",
    "        state = torch.tensor(state, device=self.device, dtype=torch.float32).unsqueeze(dim=0)\n",
    "        mu, log_std = self.actor(state)\n",
    "        mean = mu.squeeze(0)*self.action_scale + self.action_bias\n",
    "        std = log_std.exp().squeeze(0)\n",
    "        dist = Normal(mean, std)\n",
    "        action = dist.sample()\n",
    "        self.log_probs = dist.log_prob(action).detach()\n",
    "        # action = action * self.action_scale + self.action_bias\n",
    "        action = action.detach().cpu().numpy()\n",
    "        # if action >2 or action < -2:\n",
    "        #     print(mu,log_std,self.action_scale,self.action_bias,action)\n",
    "        return action\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def predict_action(self,state):\n",
    "        state = torch.tensor(state, device=self.device, dtype=torch.float32).unsqueeze(dim=0)\n",
    "        mu, log_std = self.actor(state)\n",
    "        mean = mu.squeeze(0)*self.action_scale + self.action_bias\n",
    "        std = log_std.exp().squeeze(0)\n",
    "        dist = Normal(mean, std)\n",
    "        action = dist.sample()\n",
    "        self.log_probs = dist.log_prob(action).detach()\n",
    "        # action = action * self.action_scale + self.action_bias\n",
    "        action = action.detach().cpu().numpy()\n",
    "        return action\n",
    "    \n",
    "    def update(self):\n",
    "        # update policy every n steps\n",
    "        if self.memory.sample(self.batch_size) == None:\n",
    "            return\n",
    "        \n",
    "        old_states, old_actions, old_log_probs, old_rewards, old_dones = self.memory.sample(self.batch_size)\n",
    "        \n",
    "        returns = []\n",
    "        discounted_sum = 0\n",
    "        for reward, done in zip(reversed(old_rewards), reversed(old_dones)):\n",
    "            discounted_sum = reward + (self.gamma * discounted_sum * (1 - done))\n",
    "            # if done:\n",
    "            #     discounted_sum = 0\n",
    "            # discounted_sum = reward + (self.gamma * discounted_sum)\n",
    "            returns.insert(0, discounted_sum)\n",
    "            \n",
    "        old_states = [torch.tensor(s, device=self.device, dtype=torch.float32) for s in old_states]\n",
    "        old_states = torch.cat(old_states)\n",
    "        old_actions = [torch.tensor(a, device=self.device, dtype=torch.float32) for a in old_actions]\n",
    "        old_actions = torch.cat(old_actions)\n",
    "        old_log_probs = [torch.tensor(lp, device=self.device, dtype=torch.float32) for lp in old_log_probs]\n",
    "        old_log_probs = torch.cat(old_log_probs)\n",
    "        returns = [torch.tensor(r, device=self.device, dtype=torch.float32) for r in returns]\n",
    "        returns = torch.cat(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-5) # 1e-5 to avoid division by zero\n",
    "        returns = returns.unsqueeze(dim=1)\n",
    "        # convert to tensor\n",
    "        old_states = torch.tensor(np.array(old_states), device=self.device, dtype=torch.float32)\n",
    "        old_actions = torch.tensor(np.array(old_actions), device=self.device, dtype=torch.float32)\n",
    "        old_log_probs = torch.tensor(old_log_probs, device=self.device, dtype=torch.float32).unsqueeze(dim=1)\n",
    "        old_actions = (old_actions - self.action_bias) / self.action_scale\n",
    "        # monte carlo estimate of state rewards\n",
    "        \n",
    "        # Normalizing the rewards:\n",
    "        returns = torch.tensor(returns, device=self.device, dtype=torch.float32)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-5) # 1e-5 to avoid division by zero\n",
    "        returns = returns.unsqueeze(dim=1)\n",
    "        # print(old_states.shape,old_actions.shape,old_log_probs.shape,returns.shape)\n",
    "        for _ in range(self.k_epochs):\n",
    "            # compute advantage\n",
    "            values = self.critic(old_states) # detach to avoid backprop through the critic\n",
    "            advantage = returns - values.detach()\n",
    "            # get action probabilities\n",
    "            mu, log_std = self.actor(old_states)\n",
    "            dist = Normal(mu.squeeze(1), log_std.squeeze(1))\n",
    "            # get new action probabilities\n",
    "            new_log_probs = dist.log_prob(old_actions.squeeze(1)).unsqueeze(dim=1)\n",
    "            # compute ratio (pi_theta / pi_theta__old):\n",
    "            ratio = torch.exp(new_log_probs - old_log_probs) # old_log_probs must be detached\n",
    "            # compute surrogate loss\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * advantage\n",
    "            # compute actor loss\n",
    "            actor_loss = -torch.min(surr1, surr2).mean() + self.entropy_coef * dist.entropy().mean()\n",
    "            # compute critic loss\n",
    "            critic_loss = (returns - values).pow(2).mean()\n",
    "            # take gradient step\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            critic_loss.backward()\n",
    "            # avoid gradient explosion\n",
    "            torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 2)\n",
    "            torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 2)\n",
    "            self.actor_optimizer.step()\n",
    "            self.critic_optimizer.step()\n",
    "        self.memory.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 定义训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import gymnasium as gym\n",
    "from utils.multiprocessing_env import get_eval_reward\n",
    "def train(cfg, env, agent):\n",
    "    ''' 训练\n",
    "    '''\n",
    "    print(\"开始训练！\")\n",
    "    rewards = []  # 记录所有回合的奖励\n",
    "    test_rewards = []\n",
    "    steps = []\n",
    "    best_ep_reward = float('-inf')\n",
    "    output_agent = None\n",
    "    state, info = env.reset(seed = cfg.seed)  # 重置环境，返回初始状态\n",
    "    test_env = gym.make(cfg.env_name)\n",
    "    for i_ep in range(cfg.train_eps):\n",
    "        ep_reward = 0  # 记录一回合内的奖励\n",
    "        ep_step = 0\n",
    "        for _ in range(cfg.max_steps):\n",
    "            ep_step += 1\n",
    "            action = agent.sample_action(state)  # 选择动作\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)  # 更新环境，返回transition\n",
    "            agent.memory.push((state, action,agent.log_probs,reward,truncated))  # 保存transition\n",
    "            state = next_state  # 更新下一个状态\n",
    "            agent.update()  # 更新智能体\n",
    "            ep_reward += reward  # 累加奖励\n",
    "        if (i_ep+1)%cfg.eval_per_episode == 0:\n",
    "            test_reward = np.mean([get_eval_reward(test_env=test_env , agent=agent) for _ in range(10)])\n",
    "            test_rewards.append(test_reward)\n",
    "            if test_reward >= best_ep_reward:\n",
    "                best_ep_reward = test_reward\n",
    "                output_agent = copy.deepcopy(agent)\n",
    "                print(f\"回合：{i_ep+1}/{cfg.train_eps}，评估奖励：{test_reward:.2f}，最佳评估奖励：{best_ep_reward:.2f}，更新模型！\")\n",
    "            else:\n",
    "                print(f\"回合：{i_ep+1}/{cfg.train_eps}，评估奖励：{test_reward:.2f}，最佳评估奖励：{best_ep_reward:.2f}\")\n",
    "        steps.append(ep_step)\n",
    "        rewards.append(ep_reward)\n",
    "    env.close()\n",
    "    return output_agent,{'rewards':rewards}\n",
    "\n",
    "def test(cfg, env, agent):\n",
    "    print(\"开始测试！\")\n",
    "    rewards = []  # 记录所有回合的奖励\n",
    "    steps = []\n",
    "    for i_ep in range(cfg.test_eps):\n",
    "        ep_reward = 0  # 记录一回合内的奖励\n",
    "        ep_step = 0\n",
    "        state, info  = env.reset(seed = cfg.seed)  # 重置环境，返回初始状态\n",
    "        for _ in range(cfg.max_steps):\n",
    "            ep_step+=1\n",
    "            action = agent.predict_action(state)  # 选择动作\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)  # 更新环境，返回transition\n",
    "            state = next_state  # 更新下一个状态\n",
    "            ep_reward += reward  # 累加奖励\n",
    "            if truncated:\n",
    "                break\n",
    "        steps.append(ep_step)\n",
    "        rewards.append(ep_reward)\n",
    "        print(f\"回合：{i_ep+1}/{cfg.test_eps}，奖励：{ep_reward:.2f}\")\n",
    "    print(\"完成测试\")\n",
    "    env.close()\n",
    "    return {'rewards':rewards}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 定义环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.multiprocessing_env import create_subproc_vec_env\n",
    "\n",
    "def env_agent_config(cfg):\n",
    "    env = create_subproc_vec_env(cfg.env_name, n_envs=cfg.n_envs, seed=cfg.seed)\n",
    "    n_states = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.shape[0]\n",
    "    print(f\"状态空间维度：{n_states}，动作空间维度：{n_actions}\")\n",
    "    # 更新n_states和n_actions到cfg参数中\n",
    "    setattr(cfg, 'n_states', n_states)\n",
    "    setattr(cfg, 'n_actions', n_actions) \n",
    "    setattr(cfg, 'action_space', env.action_space) \n",
    "    agent = Agent(cfg)\n",
    "    return env,agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 设置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "class Config:\n",
    "    def __init__(self) -> None:\n",
    "        self.env_name = \"Pendulum-v1\" # 环境名字\n",
    "        self.n_envs = 16\n",
    "        self.algo_name = \"PPO\" # 算法名字\n",
    "        self.mode = \"train\" # train or test\n",
    "        self.seed = 1 # 随机种子\n",
    "        self.device = \"cpu\" # device to use\n",
    "        self.train_eps = 10000 # 训练的回合数\n",
    "        self.test_eps = 20 # 测试的回合数\n",
    "        self.max_steps = 200 # 每个回合的最大步数\n",
    "        self.eval_eps = 5 # 评估的回合数\n",
    "        self.eval_per_episode = 10 # 评估的频率\n",
    "        self.gamma = 0.99 # 折扣因子\n",
    "        self.k_epochs = 6 # 更新策略网络的次数\n",
    "        self.actor_lr = 0.0003 # actor网络的学习率\n",
    "        self.critic_lr = 0.001 # critic网络的学习率\n",
    "        self.eps_clip = 0.2 # epsilon-clip\n",
    "        self.entropy_coef = 0.01 # entropy的系数\n",
    "        self.batch_size = 20 # 更新频率\n",
    "        self.actor_hidden_dim = 256 # actor网络的隐藏层维度\n",
    "        self.critic_hidden_dim = 256 # critic网络的隐藏层维度\n",
    "\n",
    "def smooth(data, weight=0.9):  \n",
    "    '''用于平滑曲线，类似于Tensorboard中的smooth曲线\n",
    "    '''\n",
    "    last = data[0] \n",
    "    smoothed = []\n",
    "    for point in data:\n",
    "        smoothed_val = last * weight + (1 - weight) * point  # 计算平滑值\n",
    "        smoothed.append(smoothed_val)                    \n",
    "        last = smoothed_val                                \n",
    "    return smoothed\n",
    "\n",
    "def plot_rewards(rewards,cfg, tag='train'):\n",
    "    ''' 画图\n",
    "    '''\n",
    "    sns.set()\n",
    "    plt.figure()  # 创建一个图形实例，方便同时多画几个图\n",
    "    plt.title(f\"{tag}ing curve on {cfg.device} of {cfg.algo_name} for {cfg.env_name}\")\n",
    "    plt.xlabel('epsiodes')\n",
    "    plt.plot(rewards, label='rewards')\n",
    "    plt.plot(smooth(rewards), label='smoothed')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取参数\n",
    "cfg = Config() \n",
    "# 训练\n",
    "env, agent = env_agent_config(cfg)\n",
    "best_agent,res_dic = train(cfg, env, agent)\n",
    " \n",
    "plot_rewards(res_dic['rewards'], cfg, tag=\"train\")  \n",
    "# 测试\n",
    "res_dic = test(cfg, env, best_agent)\n",
    "plot_rewards(res_dic['rewards'], cfg, tag=\"test\")  # 画出结果"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('easyrl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f5a9629e9f3b9957bf68a43815f911e93447d47b3d065b6a8a04975e44c504d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
